

Floating point precision

Numbers in a computer are represented in bits, so they can only be represented so precisely, for example a number like 1/3=0.3333.... where the "threes" go on forever cannot be represented exactly on these machines, neither could pi (3.14.....). The way these numbers are typically stored is similar to scientific notation, as "floating point numbers" where part of the number represents the rough magnitude and the other represents the power. Because scientific notation is simpler to understand (decimal rather than binary) and the same principle, let's think about how it works. 

The number 5 billion can be represented as 5,000,000,000, a five followed by nine zeros, or as 5*10^9, where the 9 is telling us about the 9 zeros without writing them all out. While floating point allows computers to represent very large and very small numbers efficiently, it is still not perfect. This means that computers can accumulate small errors (floating point errors) because they don't have the precision to represent numbers exactly, this is usually not a problem if used carefully, but is something programmers need to keep in mind.